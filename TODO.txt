0.8.0 TODO: 
 
 * Reduce memory consumption in craigwatch.
	* :tracked_listings needs to be handled
	* clean up craigwatch a bit. I don't like:
			craig_report.each_search do |search|
		I prefer :
			report_summaries = craig_report.searches.collect do |search|
	* and clean up the sql. Rather then .find :first, ... , use a .find_url(url) kind of deal

  		
 	* Here's the goal :
		south_fl = CraigScrape.new 'us/fl/miami', 'us/fl/keys'
		
		south_fl.each_listing('rea', 'search/sss?query=rack') do |listing|
			listing.each_post do |post|
				next if post.date > whatever
			end
		}
		
		south_fl.each_post('rea', 'search/sss?query=rack') do
			# Not very memory efficient
			# Should sort them by relative date! (this might actually work! )
		end
		
		south_fl.posts['rea', 'search/sss?query=rack'].each do
			break if post.older_then_we_could_care_about?
		end
		
		* Though - we'd need to override that default each - b/c the default array each will load everything and then iterate...
		* We'd like a collect too
		* maybe each_post('rea') { |post| } is easiest and makes more sense (in addition to the above)
* Make the tracking_database optional. Default to sqlite and the .yml's basename.db
	* Make the report_name optional as well.
 * Go through the rdoc README and otherwise todos - 
 	* adjust the scrape_ reference(s) to use the CraigScrape.new(*paths) syntax
 * craigwatch rdoc needs big revisions
 	* lsiting became sites and listings

Post-0.7:
 * A debug_craigwatch, which shows the current progress... (pages fetched, objects in caches..)
 * Some pages are legitimate 404's and we just can't parse them no matter how hard we try - what to do about this?
 * Maybe we should make an instance out of CraigScrape.new('us/fl/south florida') kind of thing..
 * Finsih testing out that geo location todo list
 	* Test out that array-paramter to the GeoListings constructor, make sure it actually works
	* integrate it better nito craigscrape
	* It'd be nice to tell craigscrape 'us/ca' or 'us/ca/losangeles' as the scrape location
	* and maybe have 'search text" and "search section" type stuff where everything ends up scraping from there..
	* We should really cache pages if we're going to do this - and I'd say to cache the geolisting pages first...
 * Stats in the email: bytes transferred, generation time, urls scrapped, posts scrapped
 
 * You might want to parse the sub-locations in the Craigslist object. (so for miami: brw/mia/wpb)
 * You might want to parse the categories in the Craigslist object. (community/apa, cta, etc..)
 
 * It'd also be nice to run an erb over the yaml file? No, we should take some steps to DRY out the code though. 
	* Particularly with respect to the searches which the same regex for multiple searches.
	* and particularly with those searches which are usingt he same listings urls to search for different things (IE 'cta' searches)

Recheks in a week (5.11.09 was last tried)

	* This thread:
	http://sfbay.craigslist.org/forums/?ID=29345737
	Title: craigwatch does this - if you're a little handy 
	Message: 
		craigwatch and libcraigscrape are a tightly-coupled, ruby solution for (largely) unix-based systems. 
		<br>
		<br>
		Check it out here: 
		<br>
		<a target="_top" href="http://www.derosetechnologies.com/community/libcraigscrape">http://www.derosetechnologies.com/community/libcraigscrape</a>
	* http://www.craigslistwatch.com/
	* Did this actuallyt post?: http://digg.com/tech_news/Stop_wasting_money_use_Craigslist_Watch

email: 

	http://www.dostuffright.com/Craigwatch
	http://wareseeker.com/Network-Internet/Craigslist-All-City-Search-Tool-1.2.zip/8036652
	http://www.killerstartups.com/Search/craigslittlebuddy-com-multiple-city-craigslist-search
	
Scripts aggregators:
	  bigwebmaster.com 
	http://www.scripts.com/
	http://www.scriptarchive.com/
	http://www.needscripts.com/
	http://www.scriptsearch.com/
	http://www.sitescripts.com/PHP/
	http://www.scriptsbank.com/
